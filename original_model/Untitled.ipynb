{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature key: last, Modified shape: torch.Size([256, 256, 3, 3]), Original shape: torch.Size([256, 256, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 模拟Generator类中的defactorize方法\n",
    "def defactorize_modified(fact_list, feat_shape, conv, conv_combined):\n",
    "    feats = {}\n",
    "    for _key in feat_shape:\n",
    "        _shape = feat_shape[_key]\n",
    "        _cat_dim = -len(_shape)\n",
    "\n",
    "        # 分别处理并应用标准卷积到每个特征\n",
    "        processed_facts = []\n",
    "        for _fact in fact_list:\n",
    "            _processed = conv(_fact[_key])\n",
    "            processed_facts.append(_processed)\n",
    "\n",
    "        # 合并处理后的特征\n",
    "        _cat_facts = torch.cat(processed_facts, dim=_cat_dim)\n",
    "\n",
    "        # 再次应用标准卷积到合并后的特征\n",
    "        _cat_facts = conv_combined(_cat_facts)\n",
    "\n",
    "        feats[_key] = _cat_facts  # 存储处理后的特征\n",
    "    return feats\n",
    "\n",
    "def defactorize_original(fact_list, feat_shape):\n",
    "    feats = {}\n",
    "    for _key in feat_shape:\n",
    "        _shape = feat_shape[_key]\n",
    "        _cat_dim = -len(_shape)\n",
    "        _cat_facts = torch.cat([_fact[_key] for _fact in fact_list], dim=_cat_dim)\n",
    "\n",
    "        feats[_key] = _cat_facts  # 存储处理后的特征\n",
    "    return feats\n",
    "\n",
    "# 模拟输入数据\n",
    "feat_shape = {\"last\": (256, 16, 16)}\n",
    "fact_list = [{\"last\": torch.rand(256, 256, 3, 3) for _ in range(2)}]  # 模拟两个fact_list项\n",
    "\n",
    "# 创建卷积层\n",
    "conv = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=1)\n",
    "conv_combined = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=1)\n",
    "\n",
    "# 调用两个版本的defactorize方法\n",
    "output_feats_modified = defactorize_modified(fact_list, feat_shape, conv, conv_combined)\n",
    "output_feats_original = defactorize_original(fact_list, feat_shape)\n",
    "\n",
    "# 比较两个输出的形状\n",
    "for key in output_feats_modified:\n",
    "    shape_modified = output_feats_modified[key].shape\n",
    "    shape_original = output_feats_original[key].shape\n",
    "    print(f\"Feature key: {key}, Modified shape: {shape_modified}, Original shape: {shape_original}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[256, 16, 16]' is invalid for input of size 131072",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d9213a848930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# 调用两个版本的defactorize方法\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0moutput_feats_modified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefactorize_modified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0moutput_feats_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefactorize_original\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-d9213a848930>\u001b[0m in \u001b[0;36mdefactorize_modified\u001b[0;34m(fact_list, feat_shape, conv, conv_combined)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# 调整特征尺寸以匹配原始形状\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# 注意：这可能需要根据您的具体应用进行调整\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0m_cat_facts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cat_facts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mfeats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cat_facts\u001b[0m  \u001b[0;31m# 存储处理后的特征\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[256, 16, 16]' is invalid for input of size 131072"
     ]
    }
   ],
   "source": [
    "\n",
    "# 模拟Generator类中的defactorize方法\n",
    "def defactorize_modified(fact_list, feat_shape, conv, conv_combined):\n",
    "    feats = {}\n",
    "    for _key in feat_shape:\n",
    "        _shape = feat_shape[_key]\n",
    "        _cat_dim = 1  # 在特征（通道）维度上合并\n",
    "\n",
    "        processed_facts = []\n",
    "        for _fact in fact_list:\n",
    "            # 应用第一个卷积层\n",
    "            _processed = conv(_fact[_key])\n",
    "            processed_facts.append(_processed)\n",
    "\n",
    "        # 合并处理后的特征\n",
    "        _cat_facts = torch.cat(processed_facts, dim=_cat_dim)\n",
    "\n",
    "        # 应用第二个卷积层\n",
    "        _cat_facts = conv_combined(_cat_facts)\n",
    "\n",
    "        # 调整特征尺寸以匹配原始形状\n",
    "        # 注意：这可能需要根据您的具体应用进行调整\n",
    "        _cat_facts = _cat_facts.view(*_shape)\n",
    "\n",
    "        feats[_key] = _cat_facts  # 存储处理后的特征\n",
    "\n",
    "    return feats\n",
    "\n",
    "def defactorize_original(fact_list, feat_shape):\n",
    "    feats = {}\n",
    "    for _key in feat_shape:\n",
    "        _shape = feat_shape[_key]\n",
    "        _cat_dim = -len(_shape)\n",
    "        _cat_facts = torch.cat([_fact[_key] for _fact in fact_list], dim=_cat_dim)\n",
    "\n",
    "        feats[_key] = _cat_facts  # 存储处理后的特征\n",
    "    return feats\n",
    "\n",
    "# 模拟输入数据\n",
    "feat_shape = {\"last\": (256, 16, 16)}\n",
    "fact_list = [{\"last\": torch.rand(2, 256, 16, 16) for _ in range(2)}]  # 模拟两个fact_list项\n",
    "\n",
    "# 创建卷积层\n",
    "conv = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=1)\n",
    "conv_combined = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=1)\n",
    "\n",
    "# 调用两个版本的defactorize方法\n",
    "output_feats_modified = defactorize_modified(fact_list, feat_shape, conv, conv_combined)\n",
    "output_feats_original = defactorize_original(fact_list, feat_shape)\n",
    "\n",
    "# 比较两个输出的形状\n",
    "for key in output_feats_modified:\n",
    "    shape_modified = output_feats_modified[key].shape\n",
    "    shape_original = output_feats_original[key].shape\n",
    "    print(f\"Feature key: {key}, Modified shape: {shape_modified}, Original shape: {shape_original}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of last: torch.Size([2, 6, 256, 16, 16])\n",
      "Reshaped shape for convolution: torch.Size([12, 256, 16, 16])\n",
      "Shape after convolution: torch.Size([12, 256, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def defactorize_test(fact_list, conv):\n",
    "    for _fact in fact_list:\n",
    "        for _key, _val in _fact.items():\n",
    "            # 打印原始特征的形状\n",
    "            print(f\"Original shape of {_key}: {_val.shape}\")\n",
    "\n",
    "            # 调整特征为4维张量\n",
    "            _val_reshaped = _val.view(-1, *(_val.shape[-3:]))\n",
    "            print(f\"Reshaped shape for convolution: {_val_reshaped.shape}\")\n",
    "\n",
    "            # 应用卷积\n",
    "            _processed = conv(_val_reshaped)\n",
    "            print(f\"Shape after convolution: {_processed.shape}\")\n",
    "\n",
    "# 模拟输入数据\n",
    "fact_list = [{\"last\": torch.rand(2, 6, 256, 16, 16)}]  # 模拟一个fact_list项，形状为[2, 6, 256, 16, 16]\n",
    "\n",
    "# 创建卷积层\n",
    "conv = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=1)\n",
    "\n",
    "# 运行测试函数\n",
    "defactorize_test(fact_list, conv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of last: torch.Size([2, 6, 256, 16, 16])\n",
      "Reshaped shape for convolution: torch.Size([12, 256, 16, 16])\n",
      "Shape after convolution: torch.Size([12, 256, 16, 16])\n",
      "Restored shape after convolution: torch.Size([2, 6, 256, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def defactorize_test(fact_list, conv):\n",
    "    for _fact in fact_list:\n",
    "        for _key, _val in _fact.items():\n",
    "            # 打印原始特征的形状\n",
    "            print(f\"Original shape of {_key}: {_val.shape}\")\n",
    "\n",
    "            # 调整特征为4维张量\n",
    "            batch_size, num_experts, C, H, W = _val.shape\n",
    "            _val_reshaped = _val.view(-1, C, H, W)  # 将批次大小和专家数量合并为一个维度\n",
    "            print(f\"Reshaped shape for convolution: {_val_reshaped.shape}\")\n",
    "\n",
    "            # 应用卷积\n",
    "            _processed = conv(_val_reshaped)\n",
    "            print(f\"Shape after convolution: {_processed.shape}\")\n",
    "\n",
    "            # 恢复原始批次大小和专家数量的维度\n",
    "            _processed_restored = _processed.view(batch_size, num_experts, C, H, W)\n",
    "            print(f\"Restored shape after convolution: {_processed_restored.shape}\")\n",
    "\n",
    "# 模拟输入数据\n",
    "fact_list = [{\"last\": torch.rand(2, 6, 256, 16, 16)}]  # 模拟一个fact_list项，形状为[2, 6, 256, 16, 16]\n",
    "\n",
    "# 创建卷积层\n",
    "conv = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=1)\n",
    "\n",
    "# 运行测试函数\n",
    "defactorize_test(fact_list, conv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature key: last, Modified shape: torch.Size([256, 256, 3, 3]), Original shape: torch.Size([256, 256, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "fact_list = [{\"last\": torch.rand(256, 256, 3, 3) for _ in range(2)}] \n",
    "conv = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=1)\n",
    "conv_combined = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=1)\n",
    "\n",
    "def defactorize_modified(fact_list, feat_shape, conv, conv_combined):\n",
    "    feats = {}\n",
    "    for _key in feat_shape:\n",
    "        _shape = feat_shape[_key]\n",
    "        _cat_dim = -len(_shape)\n",
    "\n",
    "        # 分别处理并应用标准卷积到每个特征\n",
    "        processed_facts = []\n",
    "        for _fact in fact_list:\n",
    "            _processed = conv(_fact[_key])\n",
    "            processed_facts.append(_processed)\n",
    "\n",
    "        # 合并处理后的特征\n",
    "        _cat_facts = torch.cat(processed_facts, dim=_cat_dim)\n",
    "\n",
    "        # 再次应用标准卷积到合并后的特征\n",
    "        _cat_facts = conv_combined(_cat_facts)\n",
    "\n",
    "        feats[_key] = _cat_facts  # 存储处理后的特征\n",
    "    return feats\n",
    "\n",
    "# 测试原始的defactorize函数\n",
    "def defactorize_original(fact_list, feat_shape):\n",
    "    feats = {}\n",
    "    for _key in feat_shape:\n",
    "        _shape = feat_shape[_key]\n",
    "        _cat_dim = -len(_shape)\n",
    "        _cat_facts = torch.cat([_fact[_key] for _fact in fact_list], dim=_cat_dim)\n",
    "\n",
    "        feats[_key] = _cat_facts  # 存储处理后的特征\n",
    "    return feats\n",
    "\n",
    "# 调用两个版本的defactorize方法\n",
    "output_feats_modified = defactorize_modified(fact_list, feat_shape, conv, conv_combined)\n",
    "output_feats_original = defactorize_original(fact_list, feat_shape)\n",
    "\n",
    "# 比较两个输出的形状\n",
    "for key in output_feats_modified:\n",
    "    shape_modified = output_feats_modified[key].shape\n",
    "    shape_original = output_feats_original[key].shape\n",
    "    print(f\"Feature key: {key}, Modified shape: {shape_modified}, Original shape: {shape_original}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [256, 256, 3, 3], but got 5-dimensional input of size [8, 6, 256, 16, 16] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-4a89ae070f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# 调用两个版本的defactorize方法\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0moutput_feats_modified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefactorize_modified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0moutput_feats_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefactorize_original\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-4a89ae070f80>\u001b[0m in \u001b[0;36mdefactorize_modified\u001b[0;34m(fact_list, feat_shape, conv, conv_combined)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_fact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfact_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# 应用第一个卷积层\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0m_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_fact\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mprocessed_facts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [256, 256, 3, 3], but got 5-dimensional input of size [8, 6, 256, 16, 16] instead"
     ]
    }
   ],
   "source": [
    "fact_list = [{\"last\": torch.rand(8, 6, 256, 16, 16) for _ in range(2)}] \n",
    "def defactorize_original(fact_list, feat_shape):\n",
    "    feats = {}\n",
    "    for _key in feat_shape:\n",
    "        _shape = feat_shape[_key]\n",
    "        _cat_dim = -len(_shape)\n",
    "        _cat_facts = torch.cat([_fact[_key] for _fact in fact_list], dim=_cat_dim)\n",
    "\n",
    "        feats[_key] = _cat_facts  # 存储处理后的特征\n",
    "    return feats\n",
    "\n",
    "# 定义卷积层\n",
    "conv = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=1)\n",
    "conv_combined = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=1)\n",
    "def defactorize_modified(fact_list, feat_shape, conv, conv_combined):\n",
    "    feats = {}\n",
    "    for _key in feat_shape:\n",
    "        _shape = feat_shape[_key]\n",
    "        _cat_dim = 1  # 在特征（通道）维度上合并\n",
    "\n",
    "        processed_facts = []\n",
    "        for _fact in fact_list:\n",
    "            # 应用第一个卷积层\n",
    "            _processed = conv(_fact[_key])\n",
    "            processed_facts.append(_processed)\n",
    "\n",
    "        # 合并处理后的特征\n",
    "        _cat_facts = torch.cat(processed_facts, dim=_cat_dim)\n",
    "\n",
    "        # 应用第二个卷积层\n",
    "        _cat_facts = conv_combined(_cat_facts)\n",
    "\n",
    "        # 调整特征尺寸以匹配原始形状\n",
    "        # 注意：这可能需要根据您的具体应用进行调整\n",
    "        _cat_facts = _cat_facts.view(*_shape)\n",
    "\n",
    "        feats[_key] = _cat_facts  # 存储处理后的特征\n",
    "\n",
    "    return feats\n",
    "\n",
    "# 调用两个版本的defactorize方法\n",
    "output_feats_modified = defactorize_modified(fact_list, feat_shape, conv, conv_combined)\n",
    "output_feats_original = defactorize_original(fact_list, feat_shape)\n",
    "\n",
    "# 比较两个输出的形状\n",
    "for key in output_feats_modified:\n",
    "    shape_modified = output_feats_modified[key].shape\n",
    "    shape_original = output_feats_original[key].shape\n",
    "    print(f\"Feature key: {key}, Modified shape: {shape_modified}, Original shape: {shape_original}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
